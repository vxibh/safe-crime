{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f21830-ab63-432f-838e-5ddc5df00df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset from disk...\n",
      "Iter: 0 Avg Hinge Loss: 0.3479097259987188\n",
      "Iter: 1 Avg Hinge Loss: 0.12691136246267476\n",
      "Iter: 2 Avg Hinge Loss: 0.19241222364332944\n",
      "Iter: 3 Avg Hinge Loss: 0.06246362373513119\n",
      "Iter: 4 Avg Hinge Loss: 0.19246261714101956\n",
      "Iter: 5 Avg Hinge Loss: 0.11835125270856997\n",
      "Iter: 6 Avg Hinge Loss: 0.06225003776756309\n",
      "Iter: 7 Avg Hinge Loss: 0.20192517295962317\n",
      "Iter: 8 Avg Hinge Loss: 0.07811829100448543\n",
      "Iter: 9 Avg Hinge Loss: 0.05050357864976421\n",
      "Iter: 10 Avg Hinge Loss: 0.21081160252246084\n",
      "Iter: 11 Avg Hinge Loss: 0.11562362719178465\n",
      "Iter: 12 Avg Hinge Loss: 0.08927616203611047\n",
      "Iter: 13 Avg Hinge Loss: 0.11158828003645546\n",
      "Iter: 14 Avg Hinge Loss: 0.09564059025254384\n",
      "Iter: 15 Avg Hinge Loss: 0.09245413117220073\n",
      "Iter: 16 Avg Hinge Loss: 0.11069605182331393\n",
      "Iter: 17 Avg Hinge Loss: 0.07642610747409352\n",
      "Iter: 18 Avg Hinge Loss: 0.06494183014203712\n",
      "Iter: 19 Avg Hinge Loss: 0.0882819096484429\n",
      "Iter: 20 Avg Hinge Loss: 0.06319971491670619\n",
      "Iter: 21 Avg Hinge Loss: 0.06902955982114302\n",
      "Iter: 22 Avg Hinge Loss: 0.12831570078782584\n",
      "Iter: 23 Avg Hinge Loss: 0.0757209618006888\n",
      "Iter: 24 Avg Hinge Loss: 0.054800927558723504\n",
      "Iter: 25 Avg Hinge Loss: 0.05519333149895203\n",
      "Iter: 26 Avg Hinge Loss: 0.089568828605886\n",
      "Iter: 27 Avg Hinge Loss: 0.08174548689497695\n",
      "Iter: 28 Avg Hinge Loss: 0.11696222355745249\n",
      "Iter: 29 Avg Hinge Loss: 0.07109067453151051\n",
      "Iter: 30 Avg Hinge Loss: 0.06155741324234639\n",
      "Iter: 31 Avg Hinge Loss: 0.05361343527804636\n",
      "Iter: 32 Avg Hinge Loss: 0.05900719732600794\n",
      "Iter: 33 Avg Hinge Loss: 0.03917240222362704\n",
      "Iter: 34 Avg Hinge Loss: 0.14303523106986463\n",
      "Iter: 35 Avg Hinge Loss: 0.1883234095412062\n",
      "Iter: 36 Avg Hinge Loss: 0.08754904782012746\n",
      "Iter: 37 Avg Hinge Loss: 0.18940627879063307\n",
      "Iter: 38 Avg Hinge Loss: 0.02235299460821182\n",
      "Iter: 39 Avg Hinge Loss: 0.08496827225965013\n",
      "Iter: 40 Avg Hinge Loss: 0.024288546144385116\n",
      "Iter: 41 Avg Hinge Loss: 0.09024290529321125\n",
      "Iter: 42 Avg Hinge Loss: 0.08524965345939742\n",
      "Iter: 43 Avg Hinge Loss: 0.09985720934861106\n",
      "Iter: 44 Avg Hinge Loss: 0.056264682992069055\n",
      "Iter: 45 Avg Hinge Loss: 0.06282248127153173\n",
      "Iter: 46 Avg Hinge Loss: 0.07517575562489529\n",
      "Iter: 47 Avg Hinge Loss: 0.06196957867876772\n",
      "Iter: 48 Avg Hinge Loss: 0.10256219245043766\n",
      "Iter: 49 Avg Hinge Loss: 0.09405490790576554\n",
      " I:2000 % Correct:99.798\n",
      "Accuracy: %99.7\n",
      "False Positives: %0.59    <- privacy violation level\n",
      "False Negatives: %0.0   <- security risk level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import chardet\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "print(\"Importing dataset from disk...\")\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "# Detect encoding for spam.txt\n",
    "encoding_spam = detect_encoding('spam.txt')\n",
    "with open('spam.txt', 'r', encoding=encoding_spam, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "spam = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "# Detect encoding for ham.txt\n",
    "encoding_ham = detect_encoding('ham.txt')\n",
    "with open('ham.txt', 'r', encoding=encoding_ham, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "ham = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, positives, negatives, iterations=10, alpha=0.005, regularization_strength=0.05):\n",
    "        self.alpha = alpha\n",
    "        self.regularization_strength = regularization_strength\n",
    "        \n",
    "        # Create vocabulary\n",
    "        cnts = Counter()\n",
    "        for email in (positives + negatives):\n",
    "            for word in email:\n",
    "                cnts[word] += 1\n",
    "        \n",
    "        vocab = list(cnts.keys())\n",
    "        self.word2index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "        # Initialize weights\n",
    "        self.weights = (np.random.rand(len(vocab)) - 0.5) * 0.1\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Calculate class weights\n",
    "        self.class_weight_spam = len(positives + negatives) / (2.0 * len(positives))  # For spam (positive class)\n",
    "        self.class_weight_ham = len(positives + negatives) / (2.0 * len(negatives))  # For ham (negative class)\n",
    "        \n",
    "        # Train model\n",
    "        self.train(positives, negatives, iterations=iterations)\n",
    "    \n",
    "    def train(self, positives, negatives, iterations=10):\n",
    "        for iter in range(iterations):\n",
    "            error = 0\n",
    "            n = 0\n",
    "            for i in range(max(len(positives), len(negatives))):\n",
    "                # Train on positive and negative examples\n",
    "                error += self.learn(positives[i % len(positives)], 1, self.class_weight_spam)\n",
    "                error += self.learn(negatives[i % len(negatives)], -1, self.class_weight_ham)\n",
    "                n += 2\n",
    "\n",
    "            print(f\"Iter: {iter} Avg Hinge Loss: {error / float(n)}\")\n",
    "    \n",
    "    def predict(self, email):\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        return np.sign(pred)\n",
    "    \n",
    "    def unencrypted_predict(self, email):\n",
    "        pred = 0\n",
    "        for word in email:\n",
    "            if word in self.word2index:\n",
    "                pred += self.weights[self.word2index[word]]\n",
    "        pred += self.bias\n",
    "        return pred\n",
    "\n",
    "    def learn(self, email, target, class_weight):\n",
    "        \"\"\"\n",
    "        Learn from one example using hinge loss with class weighting.\n",
    "        \"\"\"\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if target * pred < 1:  # Misclassified or within margin\n",
    "            # Update weights and bias\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] += self.alpha * class_weight * (target - self.regularization_strength * self.weights[self.word2index[word]])\n",
    "            self.bias += self.alpha * class_weight * target\n",
    "            return class_weight * max(0, 1 - target * pred)  # Apply class weight to hinge loss\n",
    "        else:\n",
    "            # Regularization update (when the example is correctly classified)\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] -= self.alpha * self.regularization_strength * self.weights[self.word2index[word]]\n",
    "            return 0\n",
    "\n",
    "# Train the SVM model using class-weighted learning\n",
    "model = SVM(spam[0:-1000], ham[0:-1000], iterations=50, alpha=0.008, regularization_strength=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "fp = 0\n",
    "tn = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, h in enumerate(ham[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred < 0:  # Negative class, so ham\n",
    "        tn += 1\n",
    "    else:  # False positive, classified as spam\n",
    "        fp += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * tn / float(tn + fp))[0:6])\n",
    "\n",
    "for i, h in enumerate(spam[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred > 0:  # Positive class, so spam\n",
    "        tp += 1\n",
    "    else:  # False negative, classified as ham\n",
    "        fn += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "print(\"\\nAccuracy: %\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "print(\"False Positives: %\" + str(100 * fp / float(tp + fp))[0:4] + \"    <- privacy violation level\")\n",
    "print(\"False Negatives: %\" + str(100 * fn / float(tn + fn))[0:4] + \"   <- security risk level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be72e4-9d61-4285-a806-545927b78a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c578f6e-4929-475a-8fe5-0de129e1dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset from disk...\n",
      "Iter: 0 Avg Hinge Loss: 0.29077879472627793\n",
      "Iter: 1 Avg Hinge Loss: 0.09454807004125755\n",
      "Iter: 2 Avg Hinge Loss: 0.12995744746318927\n",
      "Iter: 3 Avg Hinge Loss: 0.0977860489877116\n",
      "Iter: 4 Avg Hinge Loss: 0.07241684504606379\n",
      "Iter: 5 Avg Hinge Loss: 0.0846983369206138\n",
      "Iter: 6 Avg Hinge Loss: 0.05854650476911265\n",
      "Iter: 7 Avg Hinge Loss: 0.07045641092528987\n",
      "Iter: 8 Avg Hinge Loss: 0.05221536481273749\n",
      "Iter: 9 Avg Hinge Loss: 0.05662334796952123\n",
      "Iter: 10 Avg Hinge Loss: 0.08114024841094307\n",
      "Iter: 11 Avg Hinge Loss: 0.07633703904837863\n",
      "Iter: 12 Avg Hinge Loss: 0.13610273456558522\n",
      "Iter: 13 Avg Hinge Loss: 0.0750766280094622\n",
      "Iter: 14 Avg Hinge Loss: 0.06099816194294786\n",
      "Iter: 15 Avg Hinge Loss: 0.043882465337187204\n",
      "Iter: 16 Avg Hinge Loss: 0.07622035380367476\n",
      "Iter: 17 Avg Hinge Loss: 0.05819049872486148\n",
      "Iter: 18 Avg Hinge Loss: 0.04510003060558581\n",
      "Iter: 19 Avg Hinge Loss: 0.06795549309800365\n",
      "Iter: 20 Avg Hinge Loss: 0.08861657317238193\n",
      "Iter: 21 Avg Hinge Loss: 0.041051498819031605\n",
      "Iter: 22 Avg Hinge Loss: 0.0565565562281153\n",
      "Iter: 23 Avg Hinge Loss: 0.09471446686712048\n",
      "Iter: 24 Avg Hinge Loss: 0.07190256537315676\n",
      "Iter: 25 Avg Hinge Loss: 0.05182426606848683\n",
      "Iter: 26 Avg Hinge Loss: 0.04625000148541377\n",
      "Iter: 27 Avg Hinge Loss: 0.07785473611870922\n",
      "Iter: 28 Avg Hinge Loss: 0.05010439085786686\n",
      "Iter: 29 Avg Hinge Loss: 0.09173566748343916\n",
      "Iter: 30 Avg Hinge Loss: 0.029660966651929632\n",
      "Iter: 31 Avg Hinge Loss: 0.04952202265394675\n",
      "Iter: 32 Avg Hinge Loss: 0.08134945172253272\n",
      "Iter: 33 Avg Hinge Loss: 0.052528843976027466\n",
      "Iter: 34 Avg Hinge Loss: 0.03759207194047394\n",
      "Iter: 35 Avg Hinge Loss: 0.047170716577840104\n",
      "Iter: 36 Avg Hinge Loss: 0.08641140768283646\n",
      "Iter: 37 Avg Hinge Loss: 0.04841263766072469\n",
      "Iter: 38 Avg Hinge Loss: 0.09318360579450712\n",
      "Iter: 39 Avg Hinge Loss: 0.06681454296469852\n",
      "Iter: 40 Avg Hinge Loss: 0.042429758430674896\n",
      "Iter: 41 Avg Hinge Loss: 0.08549594897190313\n",
      "Iter: 42 Avg Hinge Loss: 0.033002169345720915\n",
      "Iter: 43 Avg Hinge Loss: 0.059591649448309886\n",
      "Iter: 44 Avg Hinge Loss: 0.05553357674755881\n",
      "Iter: 45 Avg Hinge Loss: 0.030466921509039315\n",
      "Iter: 46 Avg Hinge Loss: 0.04811818001002957\n",
      "Iter: 47 Avg Hinge Loss: 0.12826219808003445\n",
      "Iter: 48 Avg Hinge Loss: 0.02697506036916807\n",
      "Iter: 49 Avg Hinge Loss: 0.07673180100331745\n",
      " I:2000 % Correct:98.653\n",
      "Accuracy: %98.65\n",
      "False Positives: %2.62    <- privacy violation level\n",
      "False Negatives: %0.0   <- security risk level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import chardet\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "print(\"Importing dataset from disk...\")\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "# Detect encoding for spam.txt\n",
    "encoding_spam = detect_encoding('spam.txt')\n",
    "with open('spam.txt', 'r', encoding=encoding_spam, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "spam = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "# Detect encoding for ham.txt\n",
    "encoding_ham = detect_encoding('ham.txt')\n",
    "with open('ham.txt', 'r', encoding=encoding_ham, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "ham = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, positives, negatives, iterations=10, alpha=0.005, regularization_strength=0.05):\n",
    "        self.alpha = alpha\n",
    "        self.regularization_strength = regularization_strength\n",
    "        \n",
    "        # Create vocabulary\n",
    "        cnts = Counter()\n",
    "        for email in (positives + negatives):\n",
    "            for word in email:\n",
    "                cnts[word] += 1\n",
    "        \n",
    "        vocab = list(cnts.keys())\n",
    "        self.word2index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "        # Initialize weights\n",
    "        self.weights = (np.random.rand(len(vocab)) - 0.5) * 0.1\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Calculate class weights\n",
    "        self.class_weight_spam = len(positives + negatives) / (2.0 * len(positives))  # For spam (positive class)\n",
    "        self.class_weight_ham = len(positives + negatives) / (2.0 * len(negatives))  # For ham (negative class)\n",
    "        \n",
    "        # Train model\n",
    "        self.train(positives, negatives, iterations=iterations)\n",
    "    \n",
    "    def train(self, positives, negatives, iterations=10):\n",
    "        for iter in range(iterations):\n",
    "            error = 0\n",
    "            n = 0\n",
    "            for i in range(max(len(positives), len(negatives))):\n",
    "                # Train on positive and negative examples\n",
    "                error += self.learn(positives[i % len(positives)], 1, self.class_weight_spam)\n",
    "                error += self.learn(negatives[i % len(negatives)], -1, self.class_weight_ham)\n",
    "                n += 2\n",
    "\n",
    "            print(f\"Iter: {iter} Avg Hinge Loss: {error / float(n)}\")\n",
    "    \n",
    "    def predict(self, email):\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        return np.sign(pred)\n",
    "    \n",
    "    def unencrypted_predict(self, email):\n",
    "        pred = 0\n",
    "        for word in email:\n",
    "            if word in self.word2index:\n",
    "                pred += self.weights[self.word2index[word]]\n",
    "        pred += self.bias\n",
    "        return pred\n",
    "\n",
    "    def learn(self, email, target, class_weight):\n",
    "        \"\"\"\n",
    "        Learn from one example using hinge loss with class weighting.\n",
    "        \"\"\"\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if target * pred < 1:  # Misclassified or within margin\n",
    "            # Update weights and bias\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] += self.alpha * class_weight * (target - self.regularization_strength * self.weights[self.word2index[word]])\n",
    "            self.bias += self.alpha * class_weight * target\n",
    "            return class_weight * max(0, 1 - target * pred)  # Apply class weight to hinge loss\n",
    "        else:\n",
    "            # Regularization update (when the example is correctly classified)\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] -= self.alpha * self.regularization_strength * self.weights[self.word2index[word]]\n",
    "            return 0\n",
    "\n",
    "# Train the SVM model using class-weighted learning\n",
    "model = SVM(spam[0:-1000], ham[0:-1000], iterations=50, alpha=0.005, regularization_strength=0.15)\n",
    "\n",
    "# Evaluate the model\n",
    "fp = 0\n",
    "tn = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, h in enumerate(ham[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred < 0:  # Negative class, so ham\n",
    "        tn += 1\n",
    "    else:  # False positive, classified as spam\n",
    "        fp += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * tn / float(tn + fp))[0:6])\n",
    "\n",
    "for i, h in enumerate(spam[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred > 0:  # Positive class, so spam\n",
    "        tp += 1\n",
    "    else:  # False negative, classified as ham\n",
    "        fn += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "print(\"\\nAccuracy: %\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "print(\"False Positives: %\" + str(100 * fp / float(tp + fp))[0:4] + \"    <- privacy violation level\")\n",
    "print(\"False Negatives: %\" + str(100 * fn / float(tn + fn))[0:4] + \"   <- security risk level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3dcdc3-40a9-4533-a9b5-80bef58d81e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf0c2a-c6b1-4ac0-84c2-4ce7e87028f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f4d8f-17ed-4c9b-a95f-ec4fb52b8d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b60378a-9044-4605-b8cc-7372eb4e9a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset from disk...\n",
      "Iter: 0 Avg Hinge Loss: 0.1548281635146259\n",
      "Iter: 1 Avg Hinge Loss: 0.0755481961485878\n",
      "Iter: 2 Avg Hinge Loss: 0.04436736824616863\n",
      "Iter: 3 Avg Hinge Loss: 0.0948758072927415\n",
      "Iter: 4 Avg Hinge Loss: 0.03886713362116495\n",
      "Iter: 5 Avg Hinge Loss: 0.031178211702105316\n",
      "Iter: 6 Avg Hinge Loss: 0.03233633997401704\n",
      "Iter: 7 Avg Hinge Loss: 0.02984675843973002\n",
      "Iter: 8 Avg Hinge Loss: 0.025423250449728912\n",
      "Iter: 9 Avg Hinge Loss: 0.028039331237342906\n",
      "Iter: 10 Avg Hinge Loss: 0.033847160887986026\n",
      "Iter: 11 Avg Hinge Loss: 0.04172742374806986\n",
      "Iter: 12 Avg Hinge Loss: 0.021596998052713014\n",
      "Iter: 13 Avg Hinge Loss: 0.04960531684035482\n",
      "Iter: 14 Avg Hinge Loss: 0.03365676055525974\n",
      "Iter: 15 Avg Hinge Loss: 0.022658128821037217\n",
      "Iter: 16 Avg Hinge Loss: 0.025096127589202738\n",
      "Iter: 17 Avg Hinge Loss: 0.03522362032440173\n",
      "Iter: 18 Avg Hinge Loss: 0.02891721378600951\n",
      "Iter: 19 Avg Hinge Loss: 0.021342859125341795\n",
      "Iter: 20 Avg Hinge Loss: 0.022508965329422172\n",
      "Iter: 21 Avg Hinge Loss: 0.017107448809688165\n",
      "Iter: 22 Avg Hinge Loss: 0.026531033166016817\n",
      "Iter: 23 Avg Hinge Loss: 0.059737842304863877\n",
      "Iter: 24 Avg Hinge Loss: 0.014628552645927155\n",
      "Iter: 25 Avg Hinge Loss: 0.03485577115079146\n",
      "Iter: 26 Avg Hinge Loss: 0.019463131718294994\n",
      "Iter: 27 Avg Hinge Loss: 0.013655719594026956\n",
      "Iter: 28 Avg Hinge Loss: 0.030997659494649808\n",
      "Iter: 29 Avg Hinge Loss: 0.027095387993065222\n",
      "Iter: 30 Avg Hinge Loss: 0.025399663424723013\n",
      "Iter: 31 Avg Hinge Loss: 0.01747558344210303\n",
      "Iter: 32 Avg Hinge Loss: 0.019382809596989185\n",
      "Iter: 33 Avg Hinge Loss: 0.031021765263072397\n",
      "Iter: 34 Avg Hinge Loss: 0.019088303474151372\n",
      "Iter: 35 Avg Hinge Loss: 0.022660990499825125\n",
      "Iter: 36 Avg Hinge Loss: 0.022485236130942957\n",
      "Iter: 37 Avg Hinge Loss: 0.017734244673509664\n",
      "Iter: 38 Avg Hinge Loss: 0.033542355817098676\n",
      "Iter: 39 Avg Hinge Loss: 0.05307501243486764\n",
      "Iter: 40 Avg Hinge Loss: 0.011431786732304092\n",
      "Iter: 41 Avg Hinge Loss: 0.019812012827468344\n",
      "Iter: 42 Avg Hinge Loss: 0.01712668002072416\n",
      "Iter: 43 Avg Hinge Loss: 0.026240261550971433\n",
      "Iter: 44 Avg Hinge Loss: 0.05613289070934118\n",
      "Iter: 45 Avg Hinge Loss: 0.03514956614986504\n",
      "Iter: 46 Avg Hinge Loss: 0.01083018530587299\n",
      "Iter: 47 Avg Hinge Loss: 0.03111507234183237\n",
      "Iter: 48 Avg Hinge Loss: 0.011600114062649032\n",
      "Iter: 49 Avg Hinge Loss: 0.04947117616614485\n",
      " I:2000 % Correct:99.999\n",
      "Accuracy: %99.9\n",
      "False Positives: %0.19    <- privacy violation level\n",
      "False Negatives: %0.0   <- security risk level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import chardet\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "print(\"Importing dataset from disk...\")\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "# Detect encoding for spam.txt\n",
    "encoding_spam = detect_encoding('spam.txt')\n",
    "with open('spam.txt', 'r', encoding=encoding_spam, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "spam = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "# Detect encoding for ham.txt\n",
    "encoding_ham = detect_encoding('ham.txt')\n",
    "with open('ham.txt', 'r', encoding=encoding_ham, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "ham = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, positives, negatives, iterations=10, alpha=0.005, regularization_strength=0.05):\n",
    "        self.alpha = alpha\n",
    "        self.regularization_strength = regularization_strength\n",
    "        \n",
    "        # Create vocabulary\n",
    "        cnts = Counter()\n",
    "        for email in (positives + negatives):\n",
    "            for word in email:\n",
    "                cnts[word] += 1\n",
    "        \n",
    "        vocab = list(cnts.keys())\n",
    "        self.word2index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "        # Initialize weights\n",
    "        self.weights = (np.random.rand(len(vocab)) - 0.5) * 0.1\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Calculate class weights\n",
    "        self.class_weight_spam = len(positives + negatives) / (2.0 * len(positives))  # For spam (positive class)\n",
    "        self.class_weight_ham = len(positives + negatives) / (2.0 * len(negatives))  # For ham (negative class)\n",
    "        \n",
    "        # Train model\n",
    "        self.train(positives, negatives, iterations=iterations)\n",
    "    \n",
    "    def train(self, positives, negatives, iterations=10):\n",
    "        for iter in range(iterations):\n",
    "            error = 0\n",
    "            n = 0\n",
    "            for i in range(max(len(positives), len(negatives))):\n",
    "                # Train on positive and negative examples\n",
    "                error += self.learn(positives[i % len(positives)], 1, self.class_weight_spam)\n",
    "                error += self.learn(negatives[i % len(negatives)], -1, self.class_weight_ham)\n",
    "                n += 2\n",
    "\n",
    "            print(f\"Iter: {iter} Avg Hinge Loss: {error / float(n)}\")\n",
    "    \n",
    "    def predict(self, email):\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        return np.sign(pred)\n",
    "    \n",
    "    def unencrypted_predict(self, email):\n",
    "        pred = 0\n",
    "        for word in email:\n",
    "            if word in self.word2index:\n",
    "                pred += self.weights[self.word2index[word]]\n",
    "        pred += self.bias\n",
    "        return pred\n",
    "\n",
    "    def learn(self, email, target, class_weight):\n",
    "        \"\"\"\n",
    "        Learn from one example using hinge loss with class weighting.\n",
    "        \"\"\"\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if target * pred < 1:  # Misclassified or within margin\n",
    "            # Update weights and bias\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] += self.alpha * class_weight * (target - self.regularization_strength * self.weights[self.word2index[word]])\n",
    "            self.bias += self.alpha * class_weight * target\n",
    "            return class_weight * max(0, 1 - target * pred)  # Apply class weight to hinge loss\n",
    "        else:\n",
    "            # Regularization update (when the example is correctly classified)\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] -= self.alpha * self.regularization_strength * self.weights[self.word2index[word]]\n",
    "            return 0\n",
    "\n",
    "# Train the SVM model using class-weighted learning\n",
    "model = SVM(spam[0:-1000], ham[0:-1000], iterations=50, alpha=0.002, regularization_strength=0.15)\n",
    "\n",
    "# Evaluate the model\n",
    "fp = 0\n",
    "tn = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, h in enumerate(ham[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred < 0:  # Negative class, so ham\n",
    "        tn += 1\n",
    "    else:  # False positive, classified as spam\n",
    "        fp += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * tn / float(tn + fp))[0:6])\n",
    "\n",
    "for i, h in enumerate(spam[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred > 0:  # Positive class, so spam\n",
    "        tp += 1\n",
    "    else:  # False negative, classified as ham\n",
    "        fn += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "print(\"\\nAccuracy: %\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "print(\"False Positives: %\" + str(100 * fp / float(tp + fp))[0:4] + \"    <- privacy violation level\")\n",
    "print(\"False Negatives: %\" + str(100 * fn / float(tn + fn))[0:4] + \"   <- security risk level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66feb191-bd0f-47a9-ab48-76c4f13c8816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
