{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f21830-ab63-432f-838e-5ddc5df00df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset from disk...\n",
      "Iter: 0 Avg Hinge Loss: 0.3479097259987188\n",
      "Iter: 1 Avg Hinge Loss: 0.12691136246267476\n",
      "Iter: 2 Avg Hinge Loss: 0.19241222364332944\n",
      "Iter: 3 Avg Hinge Loss: 0.06246362373513119\n",
      "Iter: 4 Avg Hinge Loss: 0.19246261714101956\n",
      "Iter: 5 Avg Hinge Loss: 0.11835125270856997\n",
      "Iter: 6 Avg Hinge Loss: 0.06225003776756309\n",
      "Iter: 7 Avg Hinge Loss: 0.20192517295962317\n",
      "Iter: 8 Avg Hinge Loss: 0.07811829100448543\n",
      "Iter: 9 Avg Hinge Loss: 0.05050357864976421\n",
      "Iter: 10 Avg Hinge Loss: 0.21081160252246084\n",
      "Iter: 11 Avg Hinge Loss: 0.11562362719178465\n",
      "Iter: 12 Avg Hinge Loss: 0.08927616203611047\n",
      "Iter: 13 Avg Hinge Loss: 0.11158828003645546\n",
      "Iter: 14 Avg Hinge Loss: 0.09564059025254384\n",
      "Iter: 15 Avg Hinge Loss: 0.09245413117220073\n",
      "Iter: 16 Avg Hinge Loss: 0.11069605182331393\n",
      "Iter: 17 Avg Hinge Loss: 0.07642610747409352\n",
      "Iter: 18 Avg Hinge Loss: 0.06494183014203712\n",
      "Iter: 19 Avg Hinge Loss: 0.0882819096484429\n",
      "Iter: 20 Avg Hinge Loss: 0.06319971491670619\n",
      "Iter: 21 Avg Hinge Loss: 0.06902955982114302\n",
      "Iter: 22 Avg Hinge Loss: 0.12831570078782584\n",
      "Iter: 23 Avg Hinge Loss: 0.0757209618006888\n",
      "Iter: 24 Avg Hinge Loss: 0.054800927558723504\n",
      "Iter: 25 Avg Hinge Loss: 0.05519333149895203\n",
      "Iter: 26 Avg Hinge Loss: 0.089568828605886\n",
      "Iter: 27 Avg Hinge Loss: 0.08174548689497695\n",
      "Iter: 28 Avg Hinge Loss: 0.11696222355745249\n",
      "Iter: 29 Avg Hinge Loss: 0.07109067453151051\n",
      "Iter: 30 Avg Hinge Loss: 0.06155741324234639\n",
      "Iter: 31 Avg Hinge Loss: 0.05361343527804636\n",
      "Iter: 32 Avg Hinge Loss: 0.05900719732600794\n",
      "Iter: 33 Avg Hinge Loss: 0.03917240222362704\n",
      "Iter: 34 Avg Hinge Loss: 0.14303523106986463\n",
      "Iter: 35 Avg Hinge Loss: 0.1883234095412062\n",
      "Iter: 36 Avg Hinge Loss: 0.08754904782012746\n",
      "Iter: 37 Avg Hinge Loss: 0.18940627879063307\n",
      "Iter: 38 Avg Hinge Loss: 0.02235299460821182\n",
      "Iter: 39 Avg Hinge Loss: 0.08496827225965013\n",
      "Iter: 40 Avg Hinge Loss: 0.024288546144385116\n",
      "Iter: 41 Avg Hinge Loss: 0.09024290529321125\n",
      "Iter: 42 Avg Hinge Loss: 0.08524965345939742\n",
      "Iter: 43 Avg Hinge Loss: 0.09985720934861106\n",
      "Iter: 44 Avg Hinge Loss: 0.056264682992069055\n",
      "Iter: 45 Avg Hinge Loss: 0.06282248127153173\n",
      "Iter: 46 Avg Hinge Loss: 0.07517575562489529\n",
      "Iter: 47 Avg Hinge Loss: 0.06196957867876772\n",
      "Iter: 48 Avg Hinge Loss: 0.10256219245043766\n",
      "Iter: 49 Avg Hinge Loss: 0.09405490790576554\n",
      " I:2000 % Correct:99.798\n",
      "Accuracy: %99.7\n",
      "False Positives: %0.59    <- privacy violation level\n",
      "False Negatives: %0.0   <- security risk level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import chardet\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "print(\"Importing dataset from disk...\")\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "# Detect encoding for spam.txt\n",
    "encoding_spam = detect_encoding('spam.txt')\n",
    "with open('spam.txt', 'r', encoding=encoding_spam, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "spam = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "# Detect encoding for ham.txt\n",
    "encoding_ham = detect_encoding('ham.txt')\n",
    "with open('ham.txt', 'r', encoding=encoding_ham, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "ham = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, positives, negatives, iterations=10, alpha=0.005, regularization_strength=0.05):\n",
    "        self.alpha = alpha\n",
    "        self.regularization_strength = regularization_strength\n",
    "        \n",
    "        # Create vocabulary\n",
    "        cnts = Counter()\n",
    "        for email in (positives + negatives):\n",
    "            for word in email:\n",
    "                cnts[word] += 1\n",
    "        \n",
    "        vocab = list(cnts.keys())\n",
    "        self.word2index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "        # Initialize weights\n",
    "        self.weights = (np.random.rand(len(vocab)) - 0.5) * 0.1\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Calculate class weights\n",
    "        self.class_weight_spam = len(positives + negatives) / (2.0 * len(positives))  # For spam (positive class)\n",
    "        self.class_weight_ham = len(positives + negatives) / (2.0 * len(negatives))  # For ham (negative class)\n",
    "        \n",
    "        # Train model\n",
    "        self.train(positives, negatives, iterations=iterations)\n",
    "    \n",
    "    def train(self, positives, negatives, iterations=10):\n",
    "        for iter in range(iterations):\n",
    "            error = 0\n",
    "            n = 0\n",
    "            for i in range(max(len(positives), len(negatives))):\n",
    "                # Train on positive and negative examples\n",
    "                error += self.learn(positives[i % len(positives)], 1, self.class_weight_spam)\n",
    "                error += self.learn(negatives[i % len(negatives)], -1, self.class_weight_ham)\n",
    "                n += 2\n",
    "\n",
    "            print(f\"Iter: {iter} Avg Hinge Loss: {error / float(n)}\")\n",
    "    \n",
    "    def predict(self, email):\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        return np.sign(pred)\n",
    "    \n",
    "    def unencrypted_predict(self, email):\n",
    "        pred = 0\n",
    "        for word in email:\n",
    "            if word in self.word2index:\n",
    "                pred += self.weights[self.word2index[word]]\n",
    "        pred += self.bias\n",
    "        return pred\n",
    "\n",
    "    def learn(self, email, target, class_weight):\n",
    "        \"\"\"\n",
    "        Learn from one example using hinge loss with class weighting.\n",
    "        \"\"\"\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if target * pred < 1:  # Misclassified or within margin\n",
    "            # Update weights and bias\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] += self.alpha * class_weight * (target - self.regularization_strength * self.weights[self.word2index[word]])\n",
    "            self.bias += self.alpha * class_weight * target\n",
    "            return class_weight * max(0, 1 - target * pred)  # Apply class weight to hinge loss\n",
    "        else:\n",
    "            # Regularization update (when the example is correctly classified)\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] -= self.alpha * self.regularization_strength * self.weights[self.word2index[word]]\n",
    "            return 0\n",
    "\n",
    "# Train the SVM model using class-weighted learning\n",
    "model = SVM(spam[0:-1000], ham[0:-1000], iterations=50, alpha=0.008, regularization_strength=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "fp = 0\n",
    "tn = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, h in enumerate(ham[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred < 0:  # Negative class, so ham\n",
    "        tn += 1\n",
    "    else:  # False positive, classified as spam\n",
    "        fp += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * tn / float(tn + fp))[0:6])\n",
    "\n",
    "for i, h in enumerate(spam[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred > 0:  # Positive class, so spam\n",
    "        tp += 1\n",
    "    else:  # False negative, classified as ham\n",
    "        fn += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "print(\"\\nAccuracy: %\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "print(\"False Positives: %\" + str(100 * fp / float(tp + fp))[0:4] + \"    <- privacy violation level\")\n",
    "print(\"False Negatives: %\" + str(100 * fn / float(tn + fn))[0:4] + \"   <- security risk level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be72e4-9d61-4285-a806-545927b78a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c578f6e-4929-475a-8fe5-0de129e1dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset from disk...\n",
      "Iter: 0 Avg Hinge Loss: 0.29077879472627793\n",
      "Iter: 1 Avg Hinge Loss: 0.09454807004125755\n",
      "Iter: 2 Avg Hinge Loss: 0.12995744746318927\n",
      "Iter: 3 Avg Hinge Loss: 0.0977860489877116\n",
      "Iter: 4 Avg Hinge Loss: 0.07241684504606379\n",
      "Iter: 5 Avg Hinge Loss: 0.0846983369206138\n",
      "Iter: 6 Avg Hinge Loss: 0.05854650476911265\n",
      "Iter: 7 Avg Hinge Loss: 0.07045641092528987\n",
      "Iter: 8 Avg Hinge Loss: 0.05221536481273749\n",
      "Iter: 9 Avg Hinge Loss: 0.05662334796952123\n",
      "Iter: 10 Avg Hinge Loss: 0.08114024841094307\n",
      "Iter: 11 Avg Hinge Loss: 0.07633703904837863\n",
      "Iter: 12 Avg Hinge Loss: 0.13610273456558522\n",
      "Iter: 13 Avg Hinge Loss: 0.0750766280094622\n",
      "Iter: 14 Avg Hinge Loss: 0.06099816194294786\n",
      "Iter: 15 Avg Hinge Loss: 0.043882465337187204\n",
      "Iter: 16 Avg Hinge Loss: 0.07622035380367476\n",
      "Iter: 17 Avg Hinge Loss: 0.05819049872486148\n",
      "Iter: 18 Avg Hinge Loss: 0.04510003060558581\n",
      "Iter: 19 Avg Hinge Loss: 0.06795549309800365\n",
      "Iter: 20 Avg Hinge Loss: 0.08861657317238193\n",
      "Iter: 21 Avg Hinge Loss: 0.041051498819031605\n",
      "Iter: 22 Avg Hinge Loss: 0.0565565562281153\n",
      "Iter: 23 Avg Hinge Loss: 0.09471446686712048\n",
      "Iter: 24 Avg Hinge Loss: 0.07190256537315676\n",
      "Iter: 25 Avg Hinge Loss: 0.05182426606848683\n",
      "Iter: 26 Avg Hinge Loss: 0.04625000148541377\n",
      "Iter: 27 Avg Hinge Loss: 0.07785473611870922\n",
      "Iter: 28 Avg Hinge Loss: 0.05010439085786686\n",
      "Iter: 29 Avg Hinge Loss: 0.09173566748343916\n",
      "Iter: 30 Avg Hinge Loss: 0.029660966651929632\n",
      "Iter: 31 Avg Hinge Loss: 0.04952202265394675\n",
      "Iter: 32 Avg Hinge Loss: 0.08134945172253272\n",
      "Iter: 33 Avg Hinge Loss: 0.052528843976027466\n",
      "Iter: 34 Avg Hinge Loss: 0.03759207194047394\n",
      "Iter: 35 Avg Hinge Loss: 0.047170716577840104\n",
      "Iter: 36 Avg Hinge Loss: 0.08641140768283646\n",
      "Iter: 37 Avg Hinge Loss: 0.04841263766072469\n",
      "Iter: 38 Avg Hinge Loss: 0.09318360579450712\n",
      "Iter: 39 Avg Hinge Loss: 0.06681454296469852\n",
      "Iter: 40 Avg Hinge Loss: 0.042429758430674896\n",
      "Iter: 41 Avg Hinge Loss: 0.08549594897190313\n",
      "Iter: 42 Avg Hinge Loss: 0.033002169345720915\n",
      "Iter: 43 Avg Hinge Loss: 0.059591649448309886\n",
      "Iter: 44 Avg Hinge Loss: 0.05553357674755881\n",
      "Iter: 45 Avg Hinge Loss: 0.030466921509039315\n",
      "Iter: 46 Avg Hinge Loss: 0.04811818001002957\n",
      "Iter: 47 Avg Hinge Loss: 0.12826219808003445\n",
      "Iter: 48 Avg Hinge Loss: 0.02697506036916807\n",
      "Iter: 49 Avg Hinge Loss: 0.07673180100331745\n",
      " I:2000 % Correct:98.653\n",
      "Accuracy: %98.65\n",
      "False Positives: %2.62    <- privacy violation level\n",
      "False Negatives: %0.0   <- security risk level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import chardet\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "print(\"Importing dataset from disk...\")\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "# Detect encoding for spam.txt\n",
    "encoding_spam = detect_encoding('spam.txt')\n",
    "with open('spam.txt', 'r', encoding=encoding_spam, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "spam = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "# Detect encoding for ham.txt\n",
    "encoding_ham = detect_encoding('ham.txt')\n",
    "with open('ham.txt', 'r', encoding=encoding_ham, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "ham = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, positives, negatives, iterations=10, alpha=0.005, regularization_strength=0.05):\n",
    "        self.alpha = alpha\n",
    "        self.regularization_strength = regularization_strength\n",
    "        \n",
    "        # Create vocabulary\n",
    "        cnts = Counter()\n",
    "        for email in (positives + negatives):\n",
    "            for word in email:\n",
    "                cnts[word] += 1\n",
    "        \n",
    "        vocab = list(cnts.keys())\n",
    "        self.word2index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "        # Initialize weights\n",
    "        self.weights = (np.random.rand(len(vocab)) - 0.5) * 0.1\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Calculate class weights\n",
    "        self.class_weight_spam = len(positives + negatives) / (2.0 * len(positives))  # For spam (positive class)\n",
    "        self.class_weight_ham = len(positives + negatives) / (2.0 * len(negatives))  # For ham (negative class)\n",
    "        \n",
    "        # Train model\n",
    "        self.train(positives, negatives, iterations=iterations)\n",
    "    \n",
    "    def train(self, positives, negatives, iterations=10):\n",
    "        for iter in range(iterations):\n",
    "            error = 0\n",
    "            n = 0\n",
    "            for i in range(max(len(positives), len(negatives))):\n",
    "                # Train on positive and negative examples\n",
    "                error += self.learn(positives[i % len(positives)], 1, self.class_weight_spam)\n",
    "                error += self.learn(negatives[i % len(negatives)], -1, self.class_weight_ham)\n",
    "                n += 2\n",
    "\n",
    "            print(f\"Iter: {iter} Avg Hinge Loss: {error / float(n)}\")\n",
    "    \n",
    "    def predict(self, email):\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        return np.sign(pred)\n",
    "    \n",
    "    def unencrypted_predict(self, email):\n",
    "        pred = 0\n",
    "        for word in email:\n",
    "            if word in self.word2index:\n",
    "                pred += self.weights[self.word2index[word]]\n",
    "        pred += self.bias\n",
    "        return pred\n",
    "\n",
    "    def learn(self, email, target, class_weight):\n",
    "        \"\"\"\n",
    "        Learn from one example using hinge loss with class weighting.\n",
    "        \"\"\"\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if target * pred < 1:  # Misclassified or within margin\n",
    "            # Update weights and bias\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] += self.alpha * class_weight * (target - self.regularization_strength * self.weights[self.word2index[word]])\n",
    "            self.bias += self.alpha * class_weight * target\n",
    "            return class_weight * max(0, 1 - target * pred)  # Apply class weight to hinge loss\n",
    "        else:\n",
    "            # Regularization update (when the example is correctly classified)\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] -= self.alpha * self.regularization_strength * self.weights[self.word2index[word]]\n",
    "            return 0\n",
    "\n",
    "# Train the SVM model using class-weighted learning\n",
    "model = SVM(spam[0:-1000], ham[0:-1000], iterations=50, alpha=0.005, regularization_strength=0.15)\n",
    "\n",
    "# Evaluate the model\n",
    "fp = 0\n",
    "tn = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, h in enumerate(ham[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred < 0:  # Negative class, so ham\n",
    "        tn += 1\n",
    "    else:  # False positive, classified as spam\n",
    "        fp += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * tn / float(tn + fp))[0:6])\n",
    "\n",
    "for i, h in enumerate(spam[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred > 0:  # Positive class, so spam\n",
    "        tp += 1\n",
    "    else:  # False negative, classified as ham\n",
    "        fn += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "print(\"\\nAccuracy: %\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "print(\"False Positives: %\" + str(100 * fp / float(tp + fp))[0:4] + \"    <- privacy violation level\")\n",
    "print(\"False Negatives: %\" + str(100 * fn / float(tn + fn))[0:4] + \"   <- security risk level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3dcdc3-40a9-4533-a9b5-80bef58d81e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf0c2a-c6b1-4ac0-84c2-4ce7e87028f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f4d8f-17ed-4c9b-a95f-ec4fb52b8d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b60378a-9044-4605-b8cc-7372eb4e9a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset from disk...\n",
      "Iter: 0 Avg Hinge Loss: 0.1548281635146259\n",
      "Iter: 1 Avg Hinge Loss: 0.0755481961485878\n",
      "Iter: 2 Avg Hinge Loss: 0.04436736824616863\n",
      "Iter: 3 Avg Hinge Loss: 0.0948758072927415\n",
      "Iter: 4 Avg Hinge Loss: 0.03886713362116495\n",
      "Iter: 5 Avg Hinge Loss: 0.031178211702105316\n",
      "Iter: 6 Avg Hinge Loss: 0.03233633997401704\n",
      "Iter: 7 Avg Hinge Loss: 0.02984675843973002\n",
      "Iter: 8 Avg Hinge Loss: 0.025423250449728912\n",
      "Iter: 9 Avg Hinge Loss: 0.028039331237342906\n",
      "Iter: 10 Avg Hinge Loss: 0.033847160887986026\n",
      "Iter: 11 Avg Hinge Loss: 0.04172742374806986\n",
      "Iter: 12 Avg Hinge Loss: 0.021596998052713014\n",
      "Iter: 13 Avg Hinge Loss: 0.04960531684035482\n",
      "Iter: 14 Avg Hinge Loss: 0.03365676055525974\n",
      "Iter: 15 Avg Hinge Loss: 0.022658128821037217\n",
      "Iter: 16 Avg Hinge Loss: 0.025096127589202738\n",
      "Iter: 17 Avg Hinge Loss: 0.03522362032440173\n",
      "Iter: 18 Avg Hinge Loss: 0.02891721378600951\n",
      "Iter: 19 Avg Hinge Loss: 0.021342859125341795\n",
      "Iter: 20 Avg Hinge Loss: 0.022508965329422172\n",
      "Iter: 21 Avg Hinge Loss: 0.017107448809688165\n",
      "Iter: 22 Avg Hinge Loss: 0.026531033166016817\n",
      "Iter: 23 Avg Hinge Loss: 0.059737842304863877\n",
      "Iter: 24 Avg Hinge Loss: 0.014628552645927155\n",
      "Iter: 25 Avg Hinge Loss: 0.03485577115079146\n",
      "Iter: 26 Avg Hinge Loss: 0.019463131718294994\n",
      "Iter: 27 Avg Hinge Loss: 0.013655719594026956\n",
      "Iter: 28 Avg Hinge Loss: 0.030997659494649808\n",
      "Iter: 29 Avg Hinge Loss: 0.027095387993065222\n",
      "Iter: 30 Avg Hinge Loss: 0.025399663424723013\n",
      "Iter: 31 Avg Hinge Loss: 0.01747558344210303\n",
      "Iter: 32 Avg Hinge Loss: 0.019382809596989185\n",
      "Iter: 33 Avg Hinge Loss: 0.031021765263072397\n",
      "Iter: 34 Avg Hinge Loss: 0.019088303474151372\n",
      "Iter: 35 Avg Hinge Loss: 0.022660990499825125\n",
      "Iter: 36 Avg Hinge Loss: 0.022485236130942957\n",
      "Iter: 37 Avg Hinge Loss: 0.017734244673509664\n",
      "Iter: 38 Avg Hinge Loss: 0.033542355817098676\n",
      "Iter: 39 Avg Hinge Loss: 0.05307501243486764\n",
      "Iter: 40 Avg Hinge Loss: 0.011431786732304092\n",
      "Iter: 41 Avg Hinge Loss: 0.019812012827468344\n",
      "Iter: 42 Avg Hinge Loss: 0.01712668002072416\n",
      "Iter: 43 Avg Hinge Loss: 0.026240261550971433\n",
      "Iter: 44 Avg Hinge Loss: 0.05613289070934118\n",
      "Iter: 45 Avg Hinge Loss: 0.03514956614986504\n",
      "Iter: 46 Avg Hinge Loss: 0.01083018530587299\n",
      "Iter: 47 Avg Hinge Loss: 0.03111507234183237\n",
      "Iter: 48 Avg Hinge Loss: 0.011600114062649032\n",
      "Iter: 49 Avg Hinge Loss: 0.04947117616614485\n",
      " I:2000 % Correct:99.999\n",
      "Accuracy: %99.9\n",
      "False Positives: %0.19    <- privacy violation level\n",
      "False Negatives: %0.0   <- security risk level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import chardet\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "print(\"Importing dataset from disk...\")\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "# Detect encoding for spam.txt\n",
    "encoding_spam = detect_encoding('spam.txt')\n",
    "with open('spam.txt', 'r', encoding=encoding_spam, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "spam = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "# Detect encoding for ham.txt\n",
    "encoding_ham = detect_encoding('ham.txt')\n",
    "with open('ham.txt', 'r', encoding=encoding_ham, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "ham = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, positives, negatives, iterations=10, alpha=0.005, regularization_strength=0.05):\n",
    "        self.alpha = alpha\n",
    "        self.regularization_strength = regularization_strength\n",
    "        \n",
    "        # Create vocabulary\n",
    "        cnts = Counter()\n",
    "        for email in (positives + negatives):\n",
    "            for word in email:\n",
    "                cnts[word] += 1\n",
    "        \n",
    "        vocab = list(cnts.keys())\n",
    "        self.word2index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "        # Initialize weights\n",
    "        self.weights = (np.random.rand(len(vocab)) - 0.5) * 0.1\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Calculate class weights\n",
    "        self.class_weight_spam = len(positives + negatives) / (2.0 * len(positives))  # For spam (positive class)\n",
    "        self.class_weight_ham = len(positives + negatives) / (2.0 * len(negatives))  # For ham (negative class)\n",
    "        \n",
    "        # Train model\n",
    "        self.train(positives, negatives, iterations=iterations)\n",
    "    \n",
    "    def train(self, positives, negatives, iterations=10):\n",
    "        for iter in range(iterations):\n",
    "            error = 0\n",
    "            n = 0\n",
    "            for i in range(max(len(positives), len(negatives))):\n",
    "                # Train on positive and negative examples\n",
    "                error += self.learn(positives[i % len(positives)], 1, self.class_weight_spam)\n",
    "                error += self.learn(negatives[i % len(negatives)], -1, self.class_weight_ham)\n",
    "                n += 2\n",
    "\n",
    "            print(f\"Iter: {iter} Avg Hinge Loss: {error / float(n)}\")\n",
    "    \n",
    "    def predict(self, email):\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        return np.sign(pred)\n",
    "    \n",
    "    def unencrypted_predict(self, email):\n",
    "        pred = 0\n",
    "        for word in email:\n",
    "            if word in self.word2index:\n",
    "                pred += self.weights[self.word2index[word]]\n",
    "        pred += self.bias\n",
    "        return pred\n",
    "\n",
    "    def learn(self, email, target, class_weight):\n",
    "        \"\"\"\n",
    "        Learn from one example using hinge loss with class weighting.\n",
    "        \"\"\"\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if target * pred < 1:  # Misclassified or within margin\n",
    "            # Update weights and bias\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] += self.alpha * class_weight * (target - self.regularization_strength * self.weights[self.word2index[word]])\n",
    "            self.bias += self.alpha * class_weight * target\n",
    "            return class_weight * max(0, 1 - target * pred)  # Apply class weight to hinge loss\n",
    "        else:\n",
    "            # Regularization update (when the example is correctly classified)\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] -= self.alpha * self.regularization_strength * self.weights[self.word2index[word]]\n",
    "            return 0\n",
    "\n",
    "# Train the SVM model using class-weighted learning\n",
    "model = SVM(spam[0:-1000], ham[0:-1000], iterations=50, alpha=0.002, regularization_strength=0.15)\n",
    "\n",
    "# Evaluate the model\n",
    "fp = 0\n",
    "tn = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, h in enumerate(ham[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred < 0:  # Negative class, so ham\n",
    "        tn += 1\n",
    "    else:  # False positive, classified as spam\n",
    "        fp += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * tn / float(tn + fp))[0:6])\n",
    "\n",
    "for i, h in enumerate(spam[-1000:]):\n",
    "    pred = model.predict(h)\n",
    "    if pred > 0:  # Positive class, so spam\n",
    "        tp += 1\n",
    "    else:  # False negative, classified as ham\n",
    "        fn += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "sys.stdout.write('\\r I:' + str(tn + tp + fn + fp) + \" % Correct:\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "\n",
    "print(\"\\nAccuracy: %\" + str(100 * (tn + tp) / float(tn + tp + fn + fp))[0:6])\n",
    "print(\"False Positives: %\" + str(100 * fp / float(tp + fp))[0:4] + \"    <- privacy violation level\")\n",
    "print(\"False Negatives: %\" + str(100 * fn / float(tn + fn))[0:4] + \"   <- security risk level\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac93381-7f1b-4366-a2ee-7b5045cb4912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset from disk...\n",
      "Generating keys, please wait...\n",
      "Keys generated successfully! (bit length: 64)\n",
      "\n",
      "Iter: 0 Avg Hinge Loss: 0.1548281635146259\n",
      "Iter: 1 Avg Hinge Loss: 0.0755481961485878\n",
      "Iter: 2 Avg Hinge Loss: 0.04436736824616863\n",
      "Iter: 3 Avg Hinge Loss: 0.0948758072927415\n",
      "Iter: 4 Avg Hinge Loss: 0.03886713362116495\n",
      "Iter: 5 Avg Hinge Loss: 0.031178211702105316\n",
      "Iter: 6 Avg Hinge Loss: 0.03233633997401704\n",
      "Iter: 7 Avg Hinge Loss: 0.02984675843973002\n",
      "Iter: 8 Avg Hinge Loss: 0.025423250449728912\n",
      "Iter: 9 Avg Hinge Loss: 0.028039331237342906\n",
      "Iter: 10 Avg Hinge Loss: 0.033847160887986026\n",
      "Iter: 11 Avg Hinge Loss: 0.04172742374806986\n",
      "Iter: 12 Avg Hinge Loss: 0.021596998052713014\n",
      "Iter: 13 Avg Hinge Loss: 0.04960531684035482\n",
      "Iter: 14 Avg Hinge Loss: 0.03365676055525974\n",
      "Iter: 15 Avg Hinge Loss: 0.022658128821037217\n",
      "Iter: 16 Avg Hinge Loss: 0.025096127589202738\n",
      "Iter: 17 Avg Hinge Loss: 0.03522362032440173\n",
      "Iter: 18 Avg Hinge Loss: 0.02891721378600951\n",
      "Iter: 19 Avg Hinge Loss: 0.021342859125341795\n",
      "Iter: 20 Avg Hinge Loss: 0.022508965329422172\n",
      "Iter: 21 Avg Hinge Loss: 0.017107448809688165\n",
      "Iter: 22 Avg Hinge Loss: 0.026531033166016817\n",
      "Iter: 23 Avg Hinge Loss: 0.059737842304863877\n",
      "Iter: 24 Avg Hinge Loss: 0.014628552645927155\n",
      "Iter: 25 Avg Hinge Loss: 0.03485577115079146\n",
      "Iter: 26 Avg Hinge Loss: 0.019463131718294994\n",
      "Iter: 27 Avg Hinge Loss: 0.013655719594026956\n",
      "Iter: 28 Avg Hinge Loss: 0.030997659494649808\n",
      "Iter: 29 Avg Hinge Loss: 0.027095387993065222\n",
      "Iter: 30 Avg Hinge Loss: 0.025399663424723013\n",
      "Iter: 31 Avg Hinge Loss: 0.01747558344210303\n",
      "Iter: 32 Avg Hinge Loss: 0.019382809596989185\n",
      "Iter: 33 Avg Hinge Loss: 0.031021765263072397\n",
      "Iter: 34 Avg Hinge Loss: 0.019088303474151372\n",
      "Iter: 35 Avg Hinge Loss: 0.022660990499825125\n",
      "Iter: 36 Avg Hinge Loss: 0.022485236130942957\n",
      "Iter: 37 Avg Hinge Loss: 0.017734244673509664\n",
      "Iter: 38 Avg Hinge Loss: 0.033542355817098676\n",
      "Iter: 39 Avg Hinge Loss: 0.05307501243486764\n",
      "Iter: 40 Avg Hinge Loss: 0.011431786732304092\n",
      "Iter: 41 Avg Hinge Loss: 0.019812012827468344\n",
      "Iter: 42 Avg Hinge Loss: 0.01712668002072416\n",
      "Iter: 43 Avg Hinge Loss: 0.026240261550971433\n",
      "Iter: 44 Avg Hinge Loss: 0.05613289070934118\n",
      "Iter: 45 Avg Hinge Loss: 0.03514956614986504\n",
      "Iter: 46 Avg Hinge Loss: 0.01083018530587299\n",
      "Iter: 47 Avg Hinge Loss: 0.03111507234183237\n",
      "Iter: 48 Avg Hinge Loss: 0.011600114062649032\n",
      "Iter: 49 Avg Hinge Loss: 0.04947117616614485\n",
      "Weights encrypted after training.\n",
      "I: 1 % Correct: 100.00\n",
      "I: 11 % Correct: 100.00\n",
      "I: 21 % Correct: 100.00\n",
      "I: 31 % Correct: 100.00\n",
      "I: 41 % Correct: 100.00\n",
      "I: 51 % Correct: 100.00\n",
      "I: 61 % Correct: 100.00\n",
      "I: 71 % Correct: 100.00\n",
      "I: 81 % Correct: 100.00\n",
      "I: 91 % Correct: 100.00\n",
      "I: 101 % Correct: 100.00\n",
      "I: 111 % Correct: 100.00\n",
      "I: 121 % Correct: 100.00\n",
      "I: 131 % Correct: 100.00\n",
      "I: 141 % Correct: 100.00\n",
      "I: 151 % Correct: 100.00\n",
      "I: 161 % Correct: 100.00\n",
      "I: 171 % Correct: 100.00\n",
      "I: 181 % Correct: 100.00\n",
      "I: 191 % Correct: 100.00\n",
      "I: 201 % Correct: 100.00\n",
      "I: 211 % Correct: 100.00\n",
      "I: 221 % Correct: 100.00\n",
      "I: 231 % Correct: 100.00\n",
      "I: 241 % Correct: 100.00\n",
      "I: 251 % Correct: 100.00\n",
      "I: 261 % Correct: 100.00\n",
      "I: 271 % Correct: 100.00\n",
      "I: 281 % Correct: 100.00\n",
      "I: 291 % Correct: 100.00\n",
      "I: 301 % Correct: 100.00\n",
      "I: 311 % Correct: 100.00\n",
      "I: 321 % Correct: 100.00\n",
      "I: 331 % Correct: 100.00\n",
      "I: 341 % Correct: 100.00\n",
      "I: 351 % Correct: 100.00\n",
      "I: 361 % Correct: 100.00\n",
      "I: 371 % Correct: 100.00\n",
      "I: 381 % Correct: 100.00\n",
      "I: 391 % Correct: 100.00\n",
      "I: 401 % Correct: 100.00\n",
      "I: 411 % Correct: 100.00\n",
      "I: 421 % Correct: 100.00\n",
      "I: 431 % Correct: 100.00\n",
      "I: 441 % Correct: 100.00\n",
      "I: 451 % Correct: 100.00\n",
      "I: 461 % Correct: 100.00\n",
      "I: 471 % Correct: 100.00\n",
      "I: 481 % Correct: 100.00\n",
      "I: 491 % Correct: 100.00\n",
      "I: 501 % Correct: 100.00\n",
      "I: 511 % Correct: 100.00\n",
      "I: 521 % Correct: 100.00\n",
      "I: 531 % Correct: 100.00\n",
      "I: 541 % Correct: 100.00\n",
      "I: 551 % Correct: 100.00\n",
      "I: 561 % Correct: 100.00\n",
      "I: 571 % Correct: 100.00\n",
      "I: 581 % Correct: 100.00\n",
      "I: 591 % Correct: 100.00\n",
      "I: 601 % Correct: 100.00\n",
      "I: 611 % Correct: 100.00\n",
      "I: 621 % Correct: 100.00\n",
      "I: 631 % Correct: 100.00\n",
      "I: 641 % Correct: 100.00\n",
      "I: 651 % Correct: 100.00\n",
      "I: 661 % Correct: 100.00\n",
      "I: 671 % Correct: 100.00\n",
      "I: 681 % Correct: 100.00\n",
      "I: 691 % Correct: 100.00\n",
      "I: 701 % Correct: 100.00\n",
      "I: 711 % Correct: 100.00\n",
      "I: 721 % Correct: 100.00\n",
      "I: 731 % Correct: 100.00\n",
      "I: 741 % Correct: 99.87\n",
      "I: 751 % Correct: 99.87\n",
      "I: 761 % Correct: 99.87\n",
      "I: 771 % Correct: 99.87\n",
      "I: 781 % Correct: 99.87\n",
      "I: 791 % Correct: 99.87\n",
      "I: 801 % Correct: 99.88\n",
      "I: 811 % Correct: 99.88\n",
      "I: 821 % Correct: 99.88\n",
      "I: 831 % Correct: 99.88\n",
      "I: 841 % Correct: 99.88\n",
      "I: 851 % Correct: 99.88\n",
      "I: 861 % Correct: 99.88\n",
      "I: 871 % Correct: 99.89\n",
      "I: 881 % Correct: 99.89\n",
      "I: 891 % Correct: 99.89\n",
      "I: 901 % Correct: 99.78\n",
      "I: 911 % Correct: 99.78\n",
      "I: 921 % Correct: 99.78\n",
      "I: 931 % Correct: 99.79\n",
      "I: 941 % Correct: 99.79\n",
      "I: 951 % Correct: 99.79\n",
      "I: 961 % Correct: 99.79\n",
      "I: 971 % Correct: 99.79\n",
      "I: 981 % Correct: 99.80\n",
      "I: 991 % Correct: 99.80\n",
      "I: 1001 % Correct: 99.80\n",
      "I: 1011 % Correct: 99.80\n",
      "I: 1021 % Correct: 99.80\n",
      "I: 1031 % Correct: 99.81\n",
      "I: 1041 % Correct: 99.81\n",
      "I: 1051 % Correct: 99.81\n",
      "I: 1061 % Correct: 99.81\n",
      "I: 1071 % Correct: 99.81\n",
      "I: 1081 % Correct: 99.81\n",
      "I: 1091 % Correct: 99.82\n",
      "I: 1101 % Correct: 99.82\n",
      "I: 1111 % Correct: 99.82\n",
      "I: 1121 % Correct: 99.82\n",
      "I: 1131 % Correct: 99.82\n",
      "I: 1141 % Correct: 99.82\n",
      "I: 1151 % Correct: 99.83\n",
      "I: 1161 % Correct: 99.83\n",
      "I: 1171 % Correct: 99.83\n",
      "I: 1181 % Correct: 99.83\n",
      "I: 1191 % Correct: 99.83\n",
      "I: 1201 % Correct: 99.83\n",
      "I: 1211 % Correct: 99.83\n",
      "I: 1221 % Correct: 99.84\n",
      "I: 1231 % Correct: 99.84\n",
      "I: 1241 % Correct: 99.84\n",
      "I: 1251 % Correct: 99.84\n",
      "I: 1261 % Correct: 99.84\n",
      "I: 1271 % Correct: 99.84\n",
      "I: 1281 % Correct: 99.84\n",
      "I: 1291 % Correct: 99.85\n",
      "I: 1301 % Correct: 99.85\n",
      "I: 1311 % Correct: 99.85\n",
      "I: 1321 % Correct: 99.85\n",
      "I: 1331 % Correct: 99.85\n",
      "I: 1341 % Correct: 99.85\n",
      "I: 1351 % Correct: 99.85\n",
      "I: 1361 % Correct: 99.85\n",
      "I: 1371 % Correct: 99.85\n",
      "I: 1381 % Correct: 99.86\n",
      "I: 1391 % Correct: 99.86\n",
      "I: 1401 % Correct: 99.86\n",
      "I: 1411 % Correct: 99.86\n",
      "I: 1421 % Correct: 99.86\n",
      "I: 1431 % Correct: 99.86\n",
      "I: 1441 % Correct: 99.86\n",
      "I: 1451 % Correct: 99.86\n",
      "I: 1461 % Correct: 99.86\n",
      "I: 1471 % Correct: 99.86\n",
      "I: 1481 % Correct: 99.86\n",
      "I: 1491 % Correct: 99.87\n",
      "I: 1501 % Correct: 99.87\n",
      "I: 1511 % Correct: 99.87\n",
      "I: 1521 % Correct: 99.87\n",
      "I: 1531 % Correct: 99.87\n",
      "I: 1541 % Correct: 99.87\n",
      "I: 1551 % Correct: 99.87\n",
      "I: 1561 % Correct: 99.87\n",
      "I: 1571 % Correct: 99.87\n",
      "I: 1581 % Correct: 99.87\n",
      "I: 1591 % Correct: 99.87\n",
      "I: 1601 % Correct: 99.88\n",
      "I: 1611 % Correct: 99.88\n",
      "I: 1621 % Correct: 99.88\n",
      "I: 1631 % Correct: 99.88\n",
      "I: 1641 % Correct: 99.88\n",
      "I: 1651 % Correct: 99.88\n",
      "I: 1661 % Correct: 99.88\n",
      "I: 1671 % Correct: 99.88\n",
      "I: 1681 % Correct: 99.88\n",
      "I: 1691 % Correct: 99.88\n",
      "I: 1701 % Correct: 99.88\n",
      "I: 1711 % Correct: 99.88\n",
      "I: 1721 % Correct: 99.88\n",
      "I: 1731 % Correct: 99.88\n",
      "I: 1741 % Correct: 99.89\n",
      "I: 1751 % Correct: 99.89\n",
      "I: 1761 % Correct: 99.89\n",
      "I: 1771 % Correct: 99.89\n",
      "I: 1781 % Correct: 99.89\n",
      "I: 1791 % Correct: 99.89\n",
      "I: 1801 % Correct: 99.89\n",
      "I: 1811 % Correct: 99.89\n",
      "I: 1821 % Correct: 99.89\n",
      "I: 1831 % Correct: 99.89\n",
      "I: 1841 % Correct: 99.89\n",
      "I: 1851 % Correct: 99.89\n",
      "I: 1861 % Correct: 99.89\n",
      "I: 1871 % Correct: 99.89\n",
      "I: 1881 % Correct: 99.89\n",
      "I: 1891 % Correct: 99.89\n",
      "I: 1901 % Correct: 99.89\n",
      "I: 1911 % Correct: 99.90\n",
      "I: 1921 % Correct: 99.90\n",
      "I: 1931 % Correct: 99.90\n",
      "I: 1941 % Correct: 99.90\n",
      "I: 1951 % Correct: 99.90\n",
      "I: 1961 % Correct: 99.90\n",
      "I: 1971 % Correct: 99.90\n",
      "I: 1981 % Correct: 99.90\n",
      "I: 1991 % Correct: 99.90\n",
      "\n",
      "Accuracy: 99.90%\n",
      "False Positives: 0.20%    <- privacy violation level\n",
      "False Negatives: 0.00%   <- security risk level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import chardet\n",
    "import math\n",
    "import re\n",
    "import libnum\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "print(\"Importing dataset from disk...\")\n",
    "\n",
    "# Function to detect the encoding of the dataset files\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding']\n",
    "\n",
    "# Detect encoding for spam.txt and ham.txt\n",
    "encoding_spam = detect_encoding('spam.txt')\n",
    "with open('spam.txt', 'r', encoding=encoding_spam, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "spam = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "encoding_ham = detect_encoding('ham.txt')\n",
    "with open('ham.txt', 'r', encoding=encoding_ham, errors='replace') as f:\n",
    "    raw = f.readlines()\n",
    "ham = [row[:-2].split(\" \") for row in raw]\n",
    "\n",
    "def mod_inverse(x, n):\n",
    "    return pow(x, -1, n)\n",
    "\n",
    "def L(x, n):\n",
    "    return (x - 1) // n\n",
    "\n",
    "def generate_keys(bit_length=256):\n",
    "    print(\"Generating keys, please wait...\")\n",
    "\n",
    "    p = libnum.generate_prime(bit_length // 2)\n",
    "    q = libnum.generate_prime(bit_length // 2)\n",
    "    n = p * q\n",
    "    n_sq = n * n\n",
    "    lambda_param = (p - 1) * (q - 1) // math.gcd(p - 1, q - 1)\n",
    "    g = n + 1\n",
    "    mu = mod_inverse(L(pow(g, lambda_param, n_sq), n), n)\n",
    "    print(f\"Keys generated successfully! (bit length: {bit_length})\\n\")\n",
    "\n",
    "    public_key = (n, g)\n",
    "    private_key = (lambda_param, mu)\n",
    "    return public_key, private_key\n",
    "\n",
    "class EncryptedNumber:\n",
    "    def __init__(self, ciphertext):\n",
    "        self.ciphertext = ciphertext\n",
    "\n",
    "def encrypt(public_key, plaintext, scaling_factor=10000):\n",
    "    n, g = public_key\n",
    "    n_sq = n * n\n",
    "    r = libnum.randint_bits(n.bit_length() - 1) % n\n",
    "    ciphertext = (pow(g, int(plaintext * scaling_factor), n_sq) * pow(r, n, n_sq)) % n_sq\n",
    "    return EncryptedNumber(ciphertext)\n",
    "\n",
    "def decrypt(private_key, public_key, encrypted_number, scaling_factor=10000):\n",
    "    n, g = public_key\n",
    "    lambda_param, mu = private_key\n",
    "    n_sq = n * n\n",
    "    ciphertext = encrypted_number.ciphertext\n",
    "    x = pow(ciphertext, lambda_param, n_sq)\n",
    "    plaintext = (L(x, n) * mu) % n\n",
    "    return plaintext / scaling_factor\n",
    "\n",
    "# Modify the SVM class to use encryption/decryption\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, positives, negatives, public_key, iterations=10, alpha=0.005, regularization_strength=0.05, weight_scale=1e5):\n",
    "        self.alpha = alpha\n",
    "        self.regularization_strength = regularization_strength\n",
    "        self.pubkey = public_key\n",
    "        self.weight_scale = weight_scale\n",
    "        \n",
    "        # Create vocabulary\n",
    "        cnts = Counter()\n",
    "        for email in (positives + negatives):\n",
    "            for word in email:\n",
    "                cnts[word] += 1\n",
    "        \n",
    "        vocab = list(cnts.keys())\n",
    "        self.word2index = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "        # Initialize weights\n",
    "        self.weights = (np.random.rand(len(vocab)) - 0.5) * 0.1\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Encrypt weights after training\n",
    "        self.encrypted_weights = [encrypt(self.pubkey, weight) for weight in self.weights]\n",
    "\n",
    "        # Calculate class weights\n",
    "        self.class_weight_spam = len(positives + negatives) / (2.0 * len(positives))  # For spam (positive class)\n",
    "        self.class_weight_ham = len(positives + negatives) / (2.0 * len(negatives))  # For ham (negative class)\n",
    "        \n",
    "        # Train model\n",
    "        self.train(positives, negatives, iterations=iterations)\n",
    "    \n",
    "    def train(self, positives, negatives, iterations=10):\n",
    "        for iter in range(iterations):\n",
    "            error = 0\n",
    "            n = 0\n",
    "            for i in range(max(len(positives), len(negatives))):\n",
    "                # Train on positive and negative examples\n",
    "                error += self.learn(positives[i % len(positives)], 1, self.class_weight_spam)\n",
    "                error += self.learn(negatives[i % len(negatives)], -1, self.class_weight_ham)\n",
    "                n += 2\n",
    "\n",
    "            print(f\"Iter: {iter} Avg Hinge Loss: {error / float(n)}\")\n",
    "    \n",
    "        # Encrypt weights after training\n",
    "        self.encrypted_weights = [encrypt(self.pubkey, weight) for weight in self.weights]\n",
    "        print(\"Weights encrypted after training.\")\n",
    "\n",
    "    def predict(self, email, encrypt_output=True):\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if encrypt_output:\n",
    "            encrypted_pred = encrypt(self.pubkey, pred)\n",
    "            return encrypted_pred\n",
    "        \n",
    "        return np.sign(pred)\n",
    "    \n",
    "    def unencrypted_predict(self, email):\n",
    "        pred = 0\n",
    "        for word in email:\n",
    "            if word in self.word2index:\n",
    "                pred += self.weights[self.word2index[word]]\n",
    "        pred += self.bias\n",
    "        return pred\n",
    "\n",
    "    def learn(self, email, target, class_weight):\n",
    "        \"\"\"\n",
    "        Learn from one example using hinge loss with class weighting.\n",
    "        \"\"\"\n",
    "        pred = self.unencrypted_predict(email)\n",
    "        if target * pred < 1:  # Misclassified or within margin\n",
    "            # Update weights and bias\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] += self.alpha * class_weight * (target - self.regularization_strength * self.weights[self.word2index[word]])\n",
    "            self.bias += self.alpha * class_weight * target\n",
    "            return class_weight * max(0, 1 - target * pred)  # Apply class weight to hinge loss\n",
    "        else:\n",
    "            # Regularization update (when the example is correctly classified)\n",
    "            for word in email:\n",
    "                if word in self.word2index:\n",
    "                    self.weights[self.word2index[word]] -= self.alpha * self.regularization_strength * self.weights[self.word2index[word]]\n",
    "            return 0\n",
    "\n",
    "# Key generation for encryption\n",
    "public_key, private_key = generate_keys(bit_length=64)\n",
    "\n",
    "# Train the SVM model using class-weighted learning and encryption\n",
    "model = SVM(spam[0:-1000], ham[0:-1000], public_key, iterations=50, alpha=0.002, regularization_strength=0.15)\n",
    "\n",
    "# Evaluate the model\n",
    "fp = 0\n",
    "tn = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "\n",
    "for i, h in enumerate(ham[-1000:]):\n",
    "    pred = model.predict(h, encrypt_output=False)\n",
    "    if pred < 0:  # Negative class, so ham\n",
    "        tn += 1\n",
    "    else:  # False positive, classified as spam\n",
    "        fp += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'I: {tn + tp + fn + fp} % Correct: {100 * tn / float(tn + fp):.2f}')\n",
    "\n",
    "for i, h in enumerate(spam[-1000:]):\n",
    "    pred = model.predict(h, encrypt_output=False)\n",
    "    if pred > 0:  # Positive class, so spam\n",
    "        tp += 1\n",
    "    else:  # False negative, classified as ham\n",
    "        fn += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'I: {tn + tp + fn + fp} % Correct: {100 * (tn + tp) / float(tn + tp + fn + fp):.2f}')\n",
    "\n",
    "print(f'\\nAccuracy: {100 * (tn + tp) / float(tn + tp + fn + fp):.2f}%')\n",
    "print(f'False Positives: {100 * fp / float(tp + fp):.2f}%    <- privacy violation level')\n",
    "print(f'False Negatives: {100 * fn / float(tn + fn):.2f}%   <- security risk level')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3672baa-bd83-404b-babb-31b4dcd7020c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EncryptedNumber at 0x10cfa1a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(spam[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b8cdf8-72f8-4e62-a63b-4cd266b47e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(spam[0], encrypt_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78c66650-cb5b-4c75-85c7-73db00dcb6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted Value with p=2, q=61728393271606572837716065728393328226711728393489337827338343: 1.0183460104133623e+58\n",
      "Success! Decrypted value: 1.0183460104133623e+58\n"
     ]
    }
   ],
   "source": [
    "import libnum\n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "def L(x, n):\n",
    "    return (x - 1) // n\n",
    "\n",
    "def mod_inverse(x, n):\n",
    "    return pow(x, -1, n)\n",
    "\n",
    "def brute_force_decrypt(public_key, ciphertext, max_key_size=512):\n",
    "    n, g = public_key\n",
    "    n_sq = n * n\n",
    "    \n",
    "    # Try different values of p and q up to a certain size\n",
    "    for p_guess in range(2, 2**max_key_size):\n",
    "        if n % p_guess == 0:  # Found a divisor of n, hence q\n",
    "            q_guess = n // p_guess\n",
    "            if p_guess * q_guess == n:\n",
    "                # We have found p and q, now calculate lambda and mu\n",
    "                lambda_param = (p_guess - 1) * (q_guess - 1) // math.gcd(p_guess - 1, q_guess - 1)\n",
    "                mu = mod_inverse(L(pow(g, lambda_param, n_sq), n), n)\n",
    "                \n",
    "                # Try decrypting the ciphertext with the guessed key\n",
    "                decrypted_value = decrypt((lambda_param, mu), public_key, ciphertext)\n",
    "                print(f\"Decrypted Value with p={p_guess}, q={q_guess}: {decrypted_value}\")\n",
    "                return decrypted_value  # Return after first success\n",
    "    return None\n",
    "\n",
    "# Example encryption to test (simulated ciphertext and public_key)\n",
    "public_key = (123456786543213145675432131456786656453423456786978675654676686, 67686543243568765434567865435678976543567865)  # Small keys for testing\n",
    "ciphertext = EncryptedNumber(2345678654321456789765432456789765432245678)  # Example ciphertext\n",
    "\n",
    "# Attempt to brute-force the encryption\n",
    "decrypted_value = brute_force_decrypt(public_key, ciphertext, max_key_size=512)\n",
    "if decrypted_value:\n",
    "    print(f\"Success! Decrypted value: {decrypted_value}\")\n",
    "else:\n",
    "    print(\"Failed to brute force the encryption.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094eed86-5c94-4257-a750-e5434a5e093b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbb9f8-3aa9-4902-a8d9-8ac2b357c77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
